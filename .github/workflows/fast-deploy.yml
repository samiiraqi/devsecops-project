name: fast-deploy

on:
  #push:
    #branches:
      #- main
      #- develop
      #- 'feature/**'
  #pull_request:
    #branches:
      #- main
      #- develop
  workflow_dispatch: {}

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY: 156041402173.dkr.ecr.us-east-1.amazonaws.com
  ECR_REPO: devsecops-project
  CLUSTER_NAME: devsecops
  NS_STAGING: devsecops-project-staging
  NS_PROD: devsecops-project-prod
  PYTHONPATH: .

permissions:
  id-token: write
  contents: read

concurrency:
  group: deploy-${{ github.ref }}
  cancel-in-progress: true

jobs:
  app-build-test-push-deploy:
    runs-on: ubuntu-latest
    outputs:
      image-tag: ${{ steps.meta.outputs.TAG }}
      namespace: ${{ steps.meta.outputs.NS }}
    steps:
      - uses: actions/checkout@v4 # Now the host runner(vm) has all our project files to work with

      # ---------- Cache dependencies ----------
      - name: Cache Python dependencies #Cache pip packages to speed up installs
        uses: actions/cache@v3 #GitHub Cache = You manually add it for extra speed
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # ---------- tests & basic security ----------
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install deps
        run: |
          python -m venv venv
          . venv/bin/activate
          pip install --upgrade pip
          [ -f requirements.txt ] && pip install -r requirements.txt
          pip install -q pytest bandit safety

      - name: Run tests
        run: |
          . venv/bin/activate
          PYTHONPATH=. python -m pytest -v --tb=short

      - name: Security scan (bandit)
        run: |
          . venv/bin/activate
          [ -d app ] && bandit -r app -ll -f json -o bandit-report.json || true
          [ -d app ] && bandit -r app -ll

      - name: Check dependencies for vulnerabilities
        run: |
          . venv/bin/activate
          safety check --json --output safety-report.json || true
          safety check

      # ---------- AWS OIDC + ECR ----------
      - name: Configure AWS (assume GitHub role)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::156041402173:role/devsecops-github-actions-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      # ---------- decide image tag & target namespace ----------
      - name: Set image tag and namespace
        id: meta
        run: |
          BRANCH="${GITHUB_REF#refs/heads/}"
          SAFE=$(echo "$BRANCH" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
          SHORT_SHA=${GITHUB_SHA:0:8}
          
          echo "SHA=${GITHUB_SHA}" >> $GITHUB_OUTPUT
          echo "SHORT_SHA=${SHORT_SHA}" >> $GITHUB_OUTPUT
          
          if [ "$BRANCH" = "main" ]; then
            echo "TAG=${SHORT_SHA}" >> $GITHUB_OUTPUT
            echo "NS=${{ env.NS_PROD }}" >> $GITHUB_OUTPUT
            echo "ENV=production" >> $GITHUB_OUTPUT
          else
            echo "TAG=${SAFE}-${SHORT_SHA}" >> $GITHUB_OUTPUT
            echo "NS=${{ env.NS_STAGING }}" >> $GITHUB_OUTPUT
            echo "ENV=staging" >> $GITHUB_OUTPUT
          fi

      # ---------- build & push ----------
      - name: Build & Push image
        run: |
          IMAGE="$ECR_REGISTRY/$ECR_REPO:${{ steps.meta.outputs.TAG }}"
          echo "Building image: $IMAGE"
          
          # Build with build args for better caching
          docker build \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            --cache-from "$ECR_REGISTRY/$ECR_REPO:cache" \
            -t "$IMAGE" \
            -t "$ECR_REGISTRY/$ECR_REPO:cache" \
            .
          
          docker push "$IMAGE"
          # Push cache layer (ignore failures)
          docker push "$ECR_REGISTRY/$ECR_REPO:cache" || true

      # ---------- kubectl & kubeconfig ----------
      - name: Install kubectl & configure
        run: |
          curl -sL "https://dl.k8s.io/release/$(curl -sL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" -o kubectl
          install -m 0755 kubectl /usr/local/bin/kubectl
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"

      # ---------- ensure namespaces exist ----------
      - name: Ensure namespaces
        run: |
          kubectl create namespace "${{ steps.meta.outputs.NS }}" --dry-run=client -o yaml | kubectl apply -f -

      # ---------- deploy application ----------
      - name: Deploy to ${{ steps.meta.outputs.ENV }}
        env:
          NS: ${{ steps.meta.outputs.NS }}
          IMAGE: ${{ env.ECR_REGISTRY }}/${{ env.ECR_REPO }}:${{ steps.meta.outputs.TAG }}
        run: |
          echo "Deploying $IMAGE to namespace $NS"
          
          # Apply deployment with image substitution
          sed "s|IMAGE_PLACEHOLDER|$IMAGE|g" k8s/deployment.yaml | kubectl -n "$NS" apply -f -
          
          # Wait for rollout to complete
          kubectl -n "$NS" rollout status deploy/python-app --timeout=600s
          
          # Verify deployment
          kubectl -n "$NS" get pods -l app=python-app

      # ---------- get service URL ----------
      - name: Get deployment URL
        id: url
        run: |
          echo "‚è≥ Waiting for service to be ready..."
          
          # Wait for service to have endpoints
          kubectl -n "${{ steps.meta.outputs.NS }}" wait --for=condition=ready --timeout=60s endpoints/python-app-service || true
          
          # Get load balancer URL with retry
          for i in {1..10}; do
            LB_URL=$(kubectl -n "${{ steps.meta.outputs.NS }}" get service python-app-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            if [ -n "$LB_URL" ]; then
              break
            fi
            echo "Attempt $i: Load balancer not ready, waiting 30s..."
            sleep 30
          done
          
          if [ -z "$LB_URL" ]; then
            LB_URL="Load balancer provisioning (may take 5-10 minutes)"
            echo "‚ö†Ô∏è Load balancer not ready yet. Check AWS console for ELB status."
          else
            echo "‚úÖ Load balancer ready!"
          fi
          
          echo "url=$LB_URL" >> $GITHUB_OUTPUT
          echo "üåê Your app will be available at: http://$LB_URL"

      # ---------- deployment summary ----------
      - name: Deployment Summary
        run: |
          cat << EOF >> $GITHUB_STEP_SUMMARY
          ## üöÄ Deployment Complete!
          
          | Property | Value |
          |----------|-------|
          | **Environment** | ${{ steps.meta.outputs.ENV }} |
          | **Namespace** | ${{ steps.meta.outputs.NS }} |
          | **Image Tag** | ${{ steps.meta.outputs.TAG }} |
          | **App URL** | http://${{ steps.url.outputs.url }} |
          | **Commit** | ${{ steps.meta.outputs.SHORT_SHA }} |
          
          ### Next Steps
          - Monitor your deployment: \`kubectl -n ${{ steps.meta.outputs.NS }} get pods\`
          - View logs: \`kubectl -n ${{ steps.meta.outputs.NS }} logs -l app=python-app\`
          - If load balancer is still provisioning, check the AWS ELB console
          EOF