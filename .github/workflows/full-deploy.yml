name: full-deploy

on:
  push:
    branches: ["main"]
    paths:
      - "infra/**"
  workflow_dispatch: {}

env:
  AWS_REGION: us-east-1
  ACCOUNT_ID: "156041402173"
  ECR_REGISTRY: 156041402173.dkr.ecr.us-east-1.amazonaws.com
  ECR_REPO: devsecops
  CLUSTER_NAME: devsecops
  K8S_NAMESPACE: devsecops
  TF_BACKEND_BUCKET: devsecops-156041402173-us-east-1
  TF_BACKEND_KEY: tfstate/infra.tfstate
  TF_LOCK_TABLE: terraform-locks
  TF_INPUT: "false"
  PYTHONPATH: .
  TERRAFORM_ROLE_ARN: arn:aws:iam::156041402173:role/devsecops-terraform-role

permissions:
  id-token: write
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  infra-build-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: "Install deps"
        shell: bash
        run: |
          python -m venv venv
          . venv/bin/activate
          pip install --upgrade pip
          [ -f requirements.txt ] && pip install -r requirements.txt || true
          pip install -q pytest bandit

      - name: "Run tests"
        shell: bash
        run: |
          . venv/bin/activate
          pytest -q

      - name: "Security scan (bandit)"
        shell: bash
        run: |
          . venv/bin/activate
          [ -d app ] && bandit -r app -ll || true

      - name: "Configure AWS (assume Terraform role)"
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TERRAFORM_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: "Setup Terraform"
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.9.5

      - name: "Ensure S3 backend bucket exists"
        shell: bash
        run: |
          set -e
          if ! aws s3api head-bucket --bucket "$TF_BACKEND_BUCKET" >/dev/null 2>&1; then
            if [ "$AWS_REGION" = "us-east-1" ]; then
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET"
            else
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET" --create-bucket-configuration LocationConstraint="$AWS_REGION"
            fi
          fi
          aws s3api put-bucket-versioning --bucket "$TF_BACKEND_BUCKET" --versioning-configuration Status=Enabled || true
          aws s3api put-public-access-block --bucket "$TF_BACKEND_BUCKET" --public-access-block-configuration '{
            "BlockPublicAcls": true, "IgnorePublicAcls": true, "BlockPublicPolicy": true, "RestrictPublicBuckets": true
          }' || true

      - name: "Ensure DynamoDB lock table exists"
        shell: bash
        run: |
          aws dynamodb describe-table --table-name "$TF_LOCK_TABLE" >/dev/null 2>&1 \
            || aws dynamodb create-table \
                 --table-name "$TF_LOCK_TABLE" \
                 --attribute-definitions AttributeName=LockID,AttributeType=S \
                 --key-schema AttributeName=LockID,KeyType=HASH \
                 --billing-mode PAY_PER_REQUEST

      - name: "Backend preflight (S3 & Dynamo)"
        shell: bash
        run: |
          set -e
          aws sts get-caller-identity
          aws s3api head-bucket --bucket "$TF_BACKEND_BUCKET"
          aws dynamodb describe-table --table-name "$TF_LOCK_TABLE" \
            --query 'Table.{Name:TableName,Status:TableStatus}' --output json

      - name: "Write backend stub (S3)"
        shell: bash
        run: |
          mkdir -p infra
          printf 'terraform {\n  backend "s3" {}\n}\n' > infra/backend.tf

      - name: "Write GitHub tfvars (auto)"
        shell: bash
        run: |
          ORG="${GITHUB_REPOSITORY_OWNER}"
          REPO="$(basename "$GITHUB_REPOSITORY")"
          {
            printf 'github_org      = "%s"\n' "$ORG"
            printf 'github_repo     = "%s"\n' "$REPO"
            printf 'github_branches = ["main"]\n'
          } > infra/terraform.auto.tfvars

      - name: "Terraform Init (S3 backend)"
        shell: bash
        run: |
          terraform -chdir=infra init -upgrade \
            -backend-config="bucket=${TF_BACKEND_BUCKET}" \
            -backend-config="key=${TF_BACKEND_KEY}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE}" \
            -backend-config="encrypt=true"

      - name: "Adopt existing VPC & subnets (idempotent)"
        shell: bash
        run: |
          set -e
          cd infra
          command -v jq >/dev/null 2>&1 || { sudo apt-get update && sudo apt-get install -y jq; }

          CIDR="10.0.0.0/16"
          VPCS_JSON=$(aws ec2 describe-vpcs \
            --filters Name=cidr-block,Values="$CIDR" Name=tag:Name,Values="devsecops,devsecops-vpc" \
            --query 'Vpcs[].{VpcId:VpcId,Name:Tags[?Key==`Name`]|[0].Value}' --output json)

          COUNT=$(echo "$VPCS_JSON" | jq 'length')
          echo "VPCs matching (by CIDR/name): $COUNT"
          imp () { terraform import "$1" "$2" >/dev/null 2>&1 || true; }

          if [ "$COUNT" -eq 1 ]; then
            VPC_ID=$(echo "$VPCS_JSON" | jq -r '.[0].VpcId')
            echo "Adopting VPC $VPC_ID"
            imp "module.vpc.aws_vpc.this" "$VPC_ID"
            for AZ in us-east-1a us-east-1b; do
              for KIND in public private; do
                NAME="devsecops-${KIND}-${AZ}"
                SID=$(aws ec2 describe-subnets --filters Name=vpc-id,Values="$VPC_ID" Name=tag:Name,Values="$NAME" \
                      --query 'Subnets[0].SubnetId' --output text || true)
                [ -n "$SID" ] && [ "$SID" != "None" ] && imp "module.vpc.aws_subnet.${KIND}[$AZ]" "$SID" || true
              done
            done
          elif [ "$COUNT" -gt 1 ]; then
            echo "::error::Found duplicate VPCs. Delete extras in AWS before continuing."
            echo "$VPCS_JSON"
            exit 1
          else
            echo "No existing VPC. Terraform will create."
          fi

      - name: "Terraform Validate"
        shell: bash
        run: terraform -chdir=infra validate
      - name: "Clear stale TF lock (best-effort)"
        shell: bash
        run: |
          aws dynamodb delete-item \
            --region "$AWS_REGION" \
            --table-name "$TF_LOCK_TABLE" \
            --key "{\"LockID\":{\"S\":\"${TF_BACKEND_BUCKET}/${TF_BACKEND_KEY}\"}}" || true
  

      - name: "Terraform Plan (save JSON)"
        shell: bash
        run: |
          terraform -chdir=infra plan -lock-timeout=10m -input=false -out=tfplan
          terraform -chdir=infra show -json tfplan > /tmp/plan.json

      - name: "FAIL if plan would create NAT/EIP (safety guard)"
        shell: bash
        run: |
          set -e
          CREATED_NAT=$(jq -r '
            [.resource_changes[]
              | select(.type=="aws_nat_gateway" or .type=="aws_eip")
              | select(.change.actions[]?=="create")
            ] | length' /tmp/plan.json)
          if [ "$CREATED_NAT" -gt 0 ]; then
            echo "::error::Plan wants to CREATE NAT/EIP ($CREATED_NAT). Aborting to prevent costs/duplicates."
            jq -r '.resource_changes[] | select(.type=="aws_nat_gateway" or .type=="aws_eip")' /tmp/plan.json || true
            exit 1
          fi
          echo "OK: no NAT/EIP creations in plan."

      - name: "Terraform Apply"
        shell: bash
        run: terraform -chdir=infra apply -lock-timeout=10m -auto-approve tfplan

      - name: "Login to ECR"
        uses: aws-actions/amazon-ecr-login@v2

      - name: "Build (if tag missing) and push image"
        env:
          IMAGE_TAG: ${{ github.sha }}
        shell: bash
        run: |
          set -e
          if aws ecr describe-images --repository-name "$ECR_REPO" --image-ids imageTag="$IMAGE_TAG" --region "$AWS_REGION" >/dev/null 2>&1; then
            echo "ECR tag $IMAGE_TAG already exists; skipping build/push."
          else
            docker build -t $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG .
            docker push $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG
          fi

      - name: "Switch to Terraform role for kubectl"
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TERRAFORM_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: "Install kubectl"
        shell: bash
        run: |
          curl -sL "https://dl.k8s.io/release/$(curl -sL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" -o kubectl
          install -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client=true

      - name: "Update kubeconfig"
        shell: bash
        run: |
          rm -f ~/.kube/config
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"
          kubectl config current-context

      - name: "Wait for at least 2 Ready nodes (up to 15m)"
        shell: bash
        run: |
          for i in {1..90}; do
            READY=$(kubectl get nodes --no-headers 2>/dev/null | awk '/ Ready/{c++} END{print c+0}')
            echo "Ready nodes: $READY"
            if [ "$READY" -ge 2 ]; then
              exit 0
            fi
            sleep 10
          done
          echo "::error::Still fewer than 2 Ready nodes."
          kubectl get nodes -o wide || true
          exit 1

      - name: "Apply manifests (k8s/)"
        shell: bash
        run: |
          kubectl apply -f k8s/

      - name: "Ensure deployment replicas=2"
        shell: bash
        run: |
          kubectl -n "$K8S_NAMESPACE" scale deploy/python-app --replicas=2

      - name: "Set image on python-app"
        env:
          IMAGE_TAG: ${{ github.sha }}
        shell: bash
        run: |
          kubectl -n "$K8S_NAMESPACE" set image deploy/python-app \
            python-app=$ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG

      - name: "Wait for rollout (up to 10m)"
        shell: bash
        run: |
          kubectl -n "$K8S_NAMESPACE" rollout status deploy/python-app --timeout=600s

      - name: "Debug: show resources / events / logs"
        if: ${{ failure() }}
        shell: bash
        run: |
          echo "==== get deploy/rs/pods ===="
          kubectl -n "$K8S_NAMESPACE" get deploy,rs,pods -o wide || true
          echo "==== describe deployment ===="
          kubectl -n "$K8S_NAMESPACE" describe deploy/python-app || true
          echo "==== describe pods (label app=python-app) ===="
          kubectl -n "$K8S_NAMESPACE" describe pods -l app=python-app || true
          echo "==== recent events ===="
          kubectl -n "$K8S_NAMESPACE" get events --sort-by=.metadata.creationTimestamp | tail -n 200 || true
          echo "==== app logs (all containers) ===="
          kubectl -n "$K8S_NAMESPACE" logs deploy/python-app --all-containers --tail=200 || true
