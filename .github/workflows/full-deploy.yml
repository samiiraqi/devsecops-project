name: full-deploy

on:
  push:
    branches: ["main"]
    paths:
      - 'infra/**'
  workflow_dispatch: {}

env:
  AWS_REGION: us-east-1
  ACCOUNT_ID: "156041402173"
  ECR_REGISTRY: 156041402173.dkr.ecr.us-east-1.amazonaws.com
  ECR_REPO: devsecops
  CLUSTER_NAME: devsecops
  K8S_NAMESPACE: devsecops
  TF_BACKEND_BUCKET: devsecops-156041402173-us-east-1
  TF_BACKEND_KEY: tfstate/infra.tfstate
  TF_LOCK_TABLE: terraform-locks
  TF_INPUT: "false"
  PYTHONPATH: .

permissions:
  id-token: write
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  infra-build-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # --------- Tests + scan ----------
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install deps
        run: |
          python -m venv venv
          . venv/bin/activate
          pip install --upgrade pip
          [ -f requirements.txt ] && pip install -r requirements.txt || true
          pip install -q pytest bandit

      - name: Run tests
        run: |
          . venv/bin/activate
          pytest -q

      - name: Security scan (bandit)
        run: |
          . venv/bin/activate
          [ -d app ] && bandit -r app -ll || true

      # --------- AWS creds -------------
      - name: Configure AWS (assume Terraform role)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::156041402173:role/devsecops-terraform-role
          aws-region: ${{ env.AWS_REGION }}

      # --------- Terraform -------------
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.9.5

      # Backend bucket/table for fresh accounts
      - name: Ensure S3 backend bucket exists
        run: |
          set -e
          if ! aws s3api head-bucket --bucket "$TF_BACKEND_BUCKET" >/dev/null 2>&1; then
            if [ "$AWS_REGION" = "us-east-1" ]; then
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET"
            else
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET" --create-bucket-configuration LocationConstraint="$AWS_REGION"
            fi
          fi
          aws s3api put-bucket-versioning --bucket "$TF_BACKEND_BUCKET" --versioning-configuration Status=Enabled || true
          aws s3api put-public-access-block --bucket "$TF_BACKEND_BUCKET" --public-access-block-configuration '{
            "BlockPublicAcls": true, "IgnorePublicAcls": true, "BlockPublicPolicy": true, "RestrictPublicBuckets": true
          }' || true

      - name: Ensure DynamoDB lock table exists
        run: |
          aws dynamodb describe-table --table-name "$TF_LOCK_TABLE" >/dev/null 2>&1 \
            || aws dynamodb create-table \
                 --table-name "$TF_LOCK_TABLE" \
                 --attribute-definitions AttributeName=LockID,AttributeType=S \
                 --key-schema AttributeName=LockID,KeyType=HASH \
                 --billing-mode PAY_PER_REQUEST

      - name: Write backend stub (S3)
        run: |
          cat > infra/backend.tf <<'HCL'
          terraform {
            backend "s3" {}
          }
          HCL

      # Provide required vars (org/repo/branches) automatically
      - name: Write GitHub tfvars (auto)
        run: |
          ORG="${GITHUB_REPOSITORY_OWNER}"
          REPO="$(basename "$GITHUB_REPOSITORY")"
          cat > infra/terraform.auto.tfvars <<EOF
          github_org      = "${ORG}"
          github_repo     = "${REPO}"
          github_branches = ["main"]
          EOF
          echo "Wrote infra/terraform.auto.tfvars with org=${ORG}, repo=${REPO}"

      - name: Terraform Init (S3 backend)
        run: |
          terraform -chdir=infra init -upgrade \
            -backend-config="bucket=${TF_BACKEND_BUCKET}" \
            -backend-config="key=${TF_BACKEND_KEY}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE}" \
            -backend-config="encrypt=true"

      # Adopt existing resources (safe if empty account)
      - name: Import pre-existing core resources (idempotent)
        run: |
          set -e
          cd infra
          imp () { terraform import "$1" "$2" >/dev/null 2>&1 || true; }

          # If bucket already exists (we just created it), let TF own it
          if aws s3api head-bucket --bucket "${{ env.TF_BACKEND_BUCKET }}" >/dev/null 2>&1; then
            imp "module.storage.aws_s3_bucket.this" "${{ env.TF_BACKEND_BUCKET }}"
          fi

          # If these exist, adopt; otherwise TF will create them
          aws ecr describe-repositories --repository-names "${{ env.ECR_REPO }}" --region "${{ env.AWS_REGION }}" >/dev/null 2>&1 && \
            imp "module.ecr.aws_ecr_repository.this" "${{ env.ECR_REPO }}" || true

          aws iam get-role --role-name "devsecops-github-actions-role" >/dev/null 2>&1 && \
            imp "module.github_oidc.aws_iam_role.github_actions" "devsecops-github-actions-role" || true

          aws iam get-role --role-name "devsecops-eks-cluster-role"  >/dev/null 2>&1 && \
            imp "module.eks.aws_iam_role.cluster" "devsecops-eks-cluster-role" || true

          aws iam get-role --role-name "devsecops-eks-cluster-node-role" >/dev/null 2>&1 && \
            imp "module.eks.aws_iam_role.node" "devsecops-eks-cluster-node-role" || true

          # EKS node group (import if it already exists)
          if aws eks describe-nodegroup --cluster-name "${{ env.CLUSTER_NAME }}" \
                 --nodegroup-name "${{ env.CLUSTER_NAME }}-ng" \
                 --region "${{ env.AWS_REGION }}" >/dev/null 2>&1; then
            imp "module.eks.aws_eks_node_group.default" "${{ env.CLUSTER_NAME }}:${{ env.CLUSTER_NAME }}-ng"
          fi

          # ✅ Unconditional imports for EKS access entries + policy associations
          AE_POLICY="arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
          imp "module.eks.aws_eks_access_entry.this[\"github_admin\"]"    "${{ env.CLUSTER_NAME }}:arn:aws:iam::${{ env.ACCOUNT_ID }}:role/devsecops-github-actions-role"
          imp "module.eks.aws_eks_access_entry.this[\"terraform_admin\"]" "${{ env.CLUSTER_NAME }}:arn:aws:iam::${{ env.ACCOUNT_ID }}:role/devsecops-terraform-role"
          imp "module.eks.aws_eks_access_policy_association.this[\"github_admin\"]"    "${{ env.CLUSTER_NAME }}#arn:aws:iam::${{ env.ACCOUNT_ID }}:role/devsecops-github-actions-role#${AE_POLICY}"
          imp "module.eks.aws_eks_access_policy_association.this[\"terraform_admin\"]" "${{ env.CLUSTER_NAME }}#arn:aws:iam::${{ env.ACCOUNT_ID }}:role/devsecops-terraform-role#${AE_POLICY}"

      # ✅ Guard against duplicate VPCs (uses jq)
      - name: Adopt existing VPC & subnets (handle duplicates safely)
        run: |
          set -e
          cd infra
          command -v jq >/dev/null 2>&1 || { sudo apt-get update && sudo apt-get install -y jq; }

          CIDR="10.0.0.0/16"
          VPCS_JSON=$(aws ec2 describe-vpcs \
            --filters Name=cidr-block,Values="$CIDR" \
                      Name=tag:Name,Values="devsecops,devsecops-vpc" \
            --query 'Vpcs[].{VpcId:VpcId,Name:Tags[?Key==`Name`]|[0].Value}' --output json)

          COUNT=$(echo "$VPCS_JSON" | jq 'length')
          echo "Found $COUNT VPC(s) with CIDR $CIDR and Name in [devsecops, devsecops-vpc]"

          imp () { terraform import "$1" "$2" >/dev/null 2>&1 || true; }

          if [ "$COUNT" -eq 0 ]; then
            echo "No existing VPC found; Terraform will create devsecops-vpc."
          elif [ "$COUNT" -eq 1 ]; then
            VPC_ID=$(echo "$VPCS_JSON" | jq -r '.[0].VpcId')
            echo "Adopting VPC $VPC_ID into TF state"
            imp "module.vpc.aws_vpc.this" "$VPC_ID"
            for AZ in us-east-1a us-east-1b; do
              for KIND in public private; do
                NAME="devsecops-${KIND}-${AZ}"
                SID=$(aws ec2 describe-subnets \
                  --filters Name=vpc-id,Values="$VPC_ID" Name=tag:Name,Values="$NAME" \
                  --query 'Subnets[0].SubnetId' --output text || true)
                [ -n "$SID" ] && [ "$SID" != "None" ] && imp "module.vpc.aws_subnet.${KIND}[$AZ]" "$SID" || true
              done
            done
          else
            echo "::error::Detected TWO VPCs (duplicate). Clean up extra VPC in AWS Console to proceed."
            echo "$VPCS_JSON"
            exit 1
          fi

      # Clear stale TF lock (if any)
      - name: Clear stale Terraform lock (DynamoDB)
        run: |
          LOCK_KEY="${TF_BACKEND_BUCKET}/${TF_BACKEND_KEY}"
          aws dynamodb delete-item --table-name "$TF_LOCK_TABLE" --key "{\"LockID\":{\"S\":\"$LOCK_KEY\"}}" >/dev/null 2>&1 || true

      # Remove leftover conflicts (KMS alias & CW log group)
      - name: Remove conflicting leftovers (KMS alias & CW log group)
        run: |
          aws kms delete-alias --alias-name "alias/eks/${CLUSTER_NAME}" >/dev/null 2>&1 || true
          aws logs delete-log-group --log-group-name "/aws/eks/${CLUSTER_NAME}/cluster" >/dev/null 2>&1 || true

      # --------- Plan & Apply ----------
      - name: Terraform Validate
        run: terraform -chdir=infra validate

      - name: Terraform Plan
        run: terraform -chdir=infra plan -lock-timeout=10m -input=false -out=tfplan

      - name: Terraform Apply
        run: terraform -chdir=infra apply -lock-timeout=10m -auto-approve tfplan

      # --------- Build & push image ----
      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push image (tag = commit SHA)
        env:
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG

      # --------- Deploy to EKS ---------

      # Force Terraform role before ANY kubectl
      - name: Switch to Terraform role for kubectl
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::156041402173:role/devsecops-terraform-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Who am I (AWS) before kubectl?
        run: aws sts get-caller-identity

      - name: Install kubectl
        run: |
          curl -sL https://dl.k8s.io/release/$(curl -sL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl -o kubectl
          install -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client=true

      - name: Wait for EKS cluster to exist
        run: |
          set -e
          echo "Waiting for EKS cluster '${CLUSTER_NAME}' to appear..."
          for i in {1..60}; do
            if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
              echo "Cluster found."
              exit 0
            fi
            echo "Not found yet... ($i/60)"
            sleep 30
          done
          echo "::error::EKS cluster '$CLUSTER_NAME' not found after waiting. Check Terraform logs."
          exit 1

      - name: Wait for EKS access entry to be active for current role
        run: |
          set -e
          ROLE_ARN=$(aws sts get-caller-identity --query Arn --output text)
          echo "Caller STS ARN: $ROLE_ARN"
          BASE_ROLE=$(echo "$ROLE_ARN" | sed -E 's#^arn:aws:sts::([0-9]+):assumed-role/([^/]+)/.*$#arn:aws:iam::\1:role/\2#')
          echo "Expecting EKS access entry for: $BASE_ROLE"
          for i in {1..40}; do
            FOUND=$(aws eks list-access-entries --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" \
              --query "accessEntries[?principalArn=='$BASE_ROLE'].principalArn" --output text || true)
            if [ "$FOUND" = "$BASE_ROLE" ]; then
              echo "Access entry present for $BASE_ROLE"
              exit 0
            fi
            echo "Waiting for access entry to propagate... ($i/40)"
            sleep 10
          done
          echo "::error::No EKS access entry found for $BASE_ROLE"
          aws eks list-access-entries --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" || true
          exit 1

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"

      - name: Sanity check RBAC (can we manage deployments?)
        run: |
          kubectl auth can-i get deployments -n "$K8S_NAMESPACE"
          kubectl auth can-i patch deployments -n "$K8S_NAMESPACE"
          kubectl auth can-i list pods -n "$K8S_NAMESPACE"

      - name: Apply manifests & set image to SHA
        run: |
          kubectl apply -f k8s/deployment.yaml
          kubectl -n "$K8S_NAMESPACE" set image deploy/python-app \
            python-app=$ECR_REGISTRY/$ECR_REPO:${{ github.sha }}

      - name: Wait for rollout
        run: |
          kubectl -n "$K8S_NAMESPACE" rollout status deploy/python-app --timeout=300s
