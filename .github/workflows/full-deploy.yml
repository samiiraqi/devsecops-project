name: full-deploy

on:
  push:
    branches: ["main"]
    paths:
      - "infra/**"
  workflow_dispatch: {}

env:
  AWS_REGION: us-east-1
  ACCOUNT_ID: "156041402173"
  ECR_REGISTRY: 156041402173.dkr.ecr.us-east-1.amazonaws.com
  ECR_REPO: devsecops
  CLUSTER_NAME: devsecops
  K8S_NAMESPACE: devsecops
  TF_BACKEND_BUCKET: devsecops-156041402173-us-east-1
  TF_BACKEND_KEY: tfstate/infra.tfstate
  TF_LOCK_TABLE: terraform-locks
  TF_INPUT: "false"
  PYTHONPATH: .
  TERRAFORM_ROLE_ARN: arn:aws:iam::156041402173:role/devsecops-terraform-role

permissions:
  id-token: write
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  infra-build-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # --------- Tests + scan ----------
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: "Install deps"
        shell: bash
        run: |
          python -m venv venv
          . venv/bin/activate
          pip install --upgrade pip
          [ -f requirements.txt ] && pip install -r requirements.txt || true
          pip install -q pytest bandit

      - name: "Run tests"
        shell: bash
        run: |
          . venv/bin/activate
          pytest -q

      - name: "Security scan (bandit)"
        shell: bash
        run: |
          . venv/bin/activate
          [ -d app ] && bandit -r app -ll || true

      # --------- AWS creds -------------
      - name: "Configure AWS (assume Terraform role)"
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TERRAFORM_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      # --------- Terraform -------------
      - name: "Setup Terraform"
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.9.5

      - name: "Ensure S3 backend bucket exists"
        shell: bash
        run: |
          set -e
          if ! aws s3api head-bucket --bucket "$TF_BACKEND_BUCKET" >/dev/null 2>&1; then
            if [ "$AWS_REGION" = "us-east-1" ]; then
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET"
            else
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET" --create-bucket-configuration LocationConstraint="$AWS_REGION"
            fi
          fi
          aws s3api put-bucket-versioning --bucket "$TF_BACKEND_BUCKET" --versioning-configuration Status=Enabled || true
          aws s3api put-public-access-block --bucket "$TF_BACKEND_BUCKET" --public-access-block-configuration '{
            "BlockPublicAcls": true, "IgnorePublicAcls": true, "BlockPublicPolicy": true, "RestrictPublicBuckets": true
          }' || true

      - name: "Ensure DynamoDB lock table exists"
        shell: bash
        run: |
          aws dynamodb describe-table --table-name "$TF_LOCK_TABLE" >/dev/null 2>&1 \
            || aws dynamodb create-table \
                 --table-name "$TF_LOCK_TABLE" \
                 --attribute-definitions AttributeName=LockID,AttributeType=S \
                 --key-schema AttributeName=LockID,KeyType=HASH \
                 --billing-mode PAY_PER_REQUEST

      - name: "Write backend stub (S3)"
        shell: bash
        run: |
          cat > infra/backend.tf <<'HCL'
          terraform {
            backend "s3" {}
          }
          HCL

      - name: "Write GitHub tfvars (auto)"
        shell: bash
        run: |
          ORG="${GITHUB_REPOSITORY_OWNER}"
          REPO="$(basename "$GITHUB_REPOSITORY")"
          cat > infra/terraform.auto.tfvars <<EOF
          github_org      = "${ORG}"
          github_repo     = "${REPO}"
          github_branches = ["main"]
          EOF
          echo "Wrote infra/terraform.auto.tfvars with org=${ORG}, repo=${REPO}"

      - name: "Terraform Init (S3 backend)"
        shell: bash
        run: |
          terraform -chdir=infra init -upgrade \
            -backend-config="bucket=${TF_BACKEND_BUCKET}" \
            -backend-config="key=${TF_BACKEND_KEY}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE}" \
            -backend-config="encrypt=true"

      - name: "Import pre-existing core resources (idempotent)"
        shell: bash
        run: |
          set -e
          cd infra
          imp () { terraform import "$1" "$2" >/dev/null 2>&1 || true; }

          # Backend bucket
          if aws s3api head-bucket --bucket "${{ env.TF_BACKEND_BUCKET }}" >/dev/null 2>&1; then
            imp "module.storage.aws_s3_bucket.this" "${{ env.TF_BACKEND_BUCKET }}"
          fi

          # ECR repo
          aws ecr describe-repositories --repository-names "${{ env.ECR_REPO }}" --region "${{ env.AWS_REGION }}" >/dev/null 2>&1 && \
            imp "module.ecr.aws_ecr_repository.this" "${{ env.ECR_REPO }}" || true

          # IAM roles
          aws iam get-role --role-name "devsecops-github-actions-role" >/dev/null 2>&1 && \
            imp "module.github_oidc.aws_iam_role.github_actions" "devsecops-github-actions-role" || true
          aws iam get-role --role-name "devsecops-eks-cluster-role"  >/dev/null 2>&1 && \
            imp "module.eks.aws_iam_role.cluster" "devsecops-eks-cluster-role" || true
          aws iam get-role --role-name "devsecops-eks-cluster-node-role" >/dev/null 2>&1 && \
            imp "module.eks.aws_iam_role.node" "devsecops-eks-cluster-node-role" || true

          # EKS node group
          if aws eks describe-nodegroup --cluster-name "${{ env.CLUSTER_NAME }}" \
                 --nodegroup-name "${{ env.CLUSTER_NAME }}-ng" \
                 --region "${{ env.AWS_REGION }}" >/dev/null 2>&1; then
            imp "module.eks.aws_eks_node_group.default" "${{ env.CLUSTER_NAME }}:${{ env.CLUSTER_NAME }}-ng"
          fi

          # EKS access entries + policy associations (import regardless)
          AE_POLICY="arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
          imp "module.eks.aws_eks_access_entry.this[\"github_admin\"]"    "${{ env.CLUSTER_NAME }}:arn:aws:iam::${{ env.ACCOUNT_ID }}:role/devsecops-github-actions-role"
          imp "module.eks.aws_eks_access_entry.this[\"terraform_admin\"]" "${{ env.CLUSTER_NAME }}:arn:aws:iam::${{ env.ACCOUNT_ID }}:role/devsecops-terraform-role"
          imp "module.eks.aws_eks_access_policy_association.this[\"github_admin\"]"    "${{ env.CLUSTER_NAME }}#arn:aws:iam::${{ env.ACCOUNT_ID }}:role/devsecops-github-actions-role#${AE_POLICY}"
          imp "module.eks.aws_eks_access_policy_association.this[\"terraform_admin\"]" "${{ env.CLUSTER_NAME }}#arn:aws:iam::${{ env.ACCOUNT_ID }}:role/devsecops-terraform-role#${AE_POLICY}"

      - name: "Adopt existing VPC & subnets (handle duplicates safely)"
        shell: bash
        run: |
          set -e
          cd infra
          command -v jq >/dev/null 2>&1 || { sudo apt-get update && sudo apt-get install -y jq; }

          CIDR="10.0.0.0/16"
          VPCS_JSON=$(aws ec2 describe-vpcs \
            --filters Name=cidr-block,Values="$CIDR" \
                      Name=tag:Name,Values="devsecops,devsecops-vpc" \
            --query 'Vpcs[].{VpcId:VpcId,Name:Tags[?Key==`Name`]|[0].Value}' --output json)

          COUNT=$(echo "$VPCS_JSON" | jq 'length')
          echo "Found $COUNT VPC(s) with CIDR $CIDR and Name in [devsecops, devsecops-vpc]"

          imp () { terraform import "$1" "$2" >/dev/null 2>&1 || true; }

          if [ "$COUNT" -eq 0 ]; then
            echo "No existing VPC found; Terraform will create devsecops-vpc."
          elif [ "$COUNT" -eq 1 ]; then
            VPC_ID=$(echo "$VPCS_JSON" | jq -r '.[0].VpcId')
            echo "Adopting VPC $VPC_ID into TF state"
            imp "module.vpc.aws_vpc.this" "$VPC_ID"
            for AZ in us-east-1a us-east-1b; do
              for KIND in public private; do
                NAME="devsecops-${KIND}-${AZ}"
                SID=$(aws ec2 describe-subnets \
                  --filters Name=vpc-id,Values="$VPC_ID" Name=tag:Name,Values="$NAME" \
                  --query 'Subnets[0].SubnetId' --output text || true)
                [ -n "$SID" ] && [ "$SID" != "None" ] && imp "module.vpc.aws_subnet.${KIND}[$AZ]" "$SID" || true
              done
            done
          else
            echo "::error::Detected TWO VPCs (duplicate). Clean up extra VPC in AWS Console to proceed."
            echo "$VPCS_JSON"
            exit 1
          fi

      - name: "Clear stale Terraform lock (DynamoDB)"
        shell: bash
        run: |
          LOCK_KEY="${TF_BACKEND_BUCKET}/${TF_BACKEND_KEY}"
          aws dynamodb delete-item --table-name "$TF_LOCK_TABLE" --key "{\"LockID\":{\"S\":\"$LOCK_KEY\"}}" >/dev/null 2>&1 || true

      - name: "Remove conflicting leftovers (KMS alias & CW log group)"
        shell: bash
        run: |
          aws kms delete-alias --alias-name "alias/eks/${CLUSTER_NAME}" >/dev/null 2>&1 || true
          aws logs delete-log-group --log-group-name "/aws/eks/${CLUSTER_NAME}/cluster" >/dev/null 2>&1 || true

      - name: "Terraform Validate"
        shell: bash
        run: terraform -chdir=infra validate

      - name: "Terraform Plan"
        shell: bash
        run: terraform -chdir=infra plan -lock-timeout=10m -input=false -out=tfplan

      - name: "Terraform Apply"
        shell: bash
        run: terraform -chdir=infra apply -lock-timeout=10m -auto-approve tfplan

      # --------- Build & push image ----
      - name: "Login to ECR"
        uses: aws-actions/amazon-ecr-login@v2

      - name: "Build (if tag missing) and push image"
        env:
          IMAGE_TAG: ${{ github.sha }}
        shell: bash
        run: |
          set -e
          if aws ecr describe-images \
               --repository-name "$ECR_REPO" \
               --image-ids imageTag="$IMAGE_TAG" \
               --region "$AWS_REGION" >/dev/null 2>&1; then
            echo "ECR tag $IMAGE_TAG already exists; skipping build/push."
          else
            docker build -t $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG .
            docker push $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG
          fi

      # --------- Deploy to EKS ---------
      - name: "Switch to Terraform role for kubectl"
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TERRAFORM_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: "Who am I (AWS) before kubectl?"
        shell: bash
        run: aws sts get-caller-identity

      - name: "Install kubectl"
        shell: bash
        run: |
          curl -sL "https://dl.k8s.io/release/$(curl -sL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" -o kubectl
          install -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client=true

      - name: "Wait for EKS cluster to exist"
        shell: bash
        run: |
          set -e
          echo "Waiting for EKS cluster '${CLUSTER_NAME}' to appear..."
          for i in {1..60}; do
            if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
              echo "Cluster found."
              exit 0
            fi
            echo "Not found yet... ($i/60)"
            sleep 30
          done
          echo "::error::EKS cluster '$CLUSTER_NAME' not found after waiting. Check Terraform logs."
          exit 1

      - name: "Update kubeconfig"
        shell: bash
        run: |
          rm -f ~/.kube/config
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"
          kubectl config current-context

      - name: "Apply manifests (k8s/ folder)"
        shell: bash
        run: |
          kubectl apply -f k8s/

      - name: "Auto-scale replicas if only 1 node is Ready"
        shell: bash
        run: |
          NODES=$(kubectl get nodes --no-headers | awk '/ Ready/{c++} END{print c+0}')
          echo "Ready nodes: $NODES"
          if [ "$NODES" -lt 2 ]; then
            echo "Only $NODES node Ready; scaling python-app to 1 replica for rollout."
            kubectl -n "$K8S_NAMESPACE" scale deploy/python-app --replicas=1
          fi

      - name: "Set image on python-app"
        env:
          IMAGE_TAG: ${{ github.sha }}
        shell: bash
        run: |
          kubectl -n "$K8S_NAMESPACE" set image deploy/python-app \
            python-app=$ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG

      - name: "Wait for rollout (up to 10m)"
        shell: bash
        run: |
          kubectl -n "$K8S_NAMESPACE" rollout status deploy/python-app --timeout=600s

      - name: "Debug: show resources / events / logs"
        if: ${{ failure() }}
        shell: bash
        run: |
          echo "==== get deploy/rs/pods ===="
          kubectl -n "$K8S_NAMESPACE" get deploy,rs,pods -o wide || true
          echo "==== describe deployment ===="
          kubectl -n "$K8S_NAMESPACE" describe deploy/python-app || true
          echo "==== describe pods (label app=python-app) ===="
          kubectl -n "$K8S_NAMESPACE" describe pods -l app=python-app || true
          echo "==== recent events ===="
          kubectl -n "$K8S_NAMESPACE" get events --sort-by=.metadata.creationTimestamp | tail -n 200 || true
          echo "==== app logs (all containers) ===="
          kubectl -n "$K8S_NAMESPACE" logs deploy/python-app --all-containers --tail=200 || true
